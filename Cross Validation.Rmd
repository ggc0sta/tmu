---
title: "Cross Validation"
author: "Gabriel Costa"
date: '2022-07-21'
output:  
  md_document:
    variant: markdown_github
---


### Intro

In this exercise, we will test the Cross Validation methodology to evaluate the performance of 3 classification algorithms: **KNN**, **Logistic Regression** and **Decision Tree**. 

The Cross Validation methodology (**N-fold Cross Validation**) partitions the date into **N** non-overlapping subsets to ensure that every observation in the dataset is used on both training and test sets.

The dataset used contains information of patients who were diagnosed with either a Malignant (M) or Benign (B) cancer.


#### 1. Importing libraries and dataset 
```{r, waerning = FALSE, message = FALSE}
library(ggplot2)
library(reshape2)
library(class)
library(gmodels)
library(caret)
library(party)

filePath <- "https://raw.githubusercontent.com/ggc0sta/tmu/main/data/prostate_cancer_dataset.csv"
pc_raw <- read.csv(filePath)
```

```{r}
head(pc_raw)
str(pc_raw)
```
#### 2. Data transformation

* Remove the **id** variable.
* Normalize the dataset to transfer the variable values to a common scale.
* Convert the **diagnosis_result** variable to factor.

```{r}
pc <- pc_raw[,-1]
pcNorm <- pc


for(i in colnames(pcNorm)){
  if((class(pcNorm[,i]) == "numeric") | (class(pcNorm[,i]) == "integer")){
      x <- pcNorm[,i]
      pcNorm[,i] <- (x - min(x)) / (max(x) - min(x))    
  }

}

pcNorm$diagnosis_result <- factor(pcNorm$diagnosis_result)
```


```{r, message=FALSE, include=FALSE}
ggplot(melt(pc[2:ncol(pcNorm)]), aes(x = value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ variable, scales = "free")

ggplot(melt(pcNorm[2:ncol(pcNorm)]), aes(x = value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~ variable, scales = "free")
```

#### 3. Cross Validation
```{r}
set.seed(1)

folds <- createFolds(pcNorm$diagnosis_result, k = 10)

fAccuracy <- data.frame(knn = numeric(10), log = numeric(10), dt = numeric(10))

for(i in 1:10){
  
  f <- folds[[i]]
  
  # 1. Dividing training & test sets
  train <- pcNorm[-f,]   
  test <- pcNorm[f,]
  
  ## i. storing the labels
  lblTrain <- train$diagnosis_result
  lblTest <- test$diagnosis_result
  
  ## ii. removing labels from train & test sets
  train_noLbl <- train[-1]
  test_noLbl <- test[-1]
  
  # 2. Modeling
  knnPredictions <- knn(train = train_noLbl, test = test_noLbl, cl = lblTrain, k = 3)
  glm1 <- glm(diagnosis_result ~., data = train, family = "binomial")
  dt <- ctree(diagnosis_result ~., data = train)
  
  # 3. Storing accuracy results
  ## i. knn
  cm <- table(lblTest, knnPredictions)
  acc <- confusionMatrix(cm, positive = "M")$overall["Accuracy"]
  fAccuracy$knn[i] <- acc 
  
  ## ii. logistic reg
  pred <- predict(glm1, newdata = test)
  pred <- ifelse(pred >= 0.5, 1, 0)
  pred <- factor(pred, labels = c("B", "M"))
  
  cm <- table(pred, test$diagnosis_result)
  acc <- confusionMatrix(cm)$overall["Accuracy"]
  fAccuracy$log[i] <- acc 
  
  ## iii. decision tree
  dtPredictions <- predict(dt, test)
  cm <- table(lblTest, dtPredictions)
  acc <- confusionMatrix(cm, positive = "M")$overall["Accuracy"]
  fAccuracy$dt[i] <- acc
}
```


#### 4. Model Evaluation
```{r, message=FALSE}
A <- melt(fAccuracy)
ggplot(A, aes(y=value, x=variable, color=variable)) +
  geom_boxplot(alpha = 0.5)
```

To check if there is a difference in model performance, we perform the Non-parametric Kruskal-Wallis test. Given the p-value of the test, we do not reject the null hypothesis that there is a difference in model performance.

```{r}
# Kruskal-Wallis test
# Hypothesis testing for more than two non-parametric variables

# H0: k populations are identical
# H1: at least 2 of the k populations differ

kruskal.test(formula = value ~ variable, data = A) 

```
